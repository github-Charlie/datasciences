<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width">
		<meta name="description" content="MyWorld of programming from data to solution">
		<meta name="keywords" content="web design, html, css, javascript, python, R, SQL, big data, hadoop, data science, data analysis, data mining, machine learning, deep learning, artificial intelligence, al">
		<meta name="author" content="C.H.">
		<meta name="copyright" content="&copy; 2018 data sciences">
		<meta name="date" content="2018-01-10" scheme="YYYY-MM-DD">
		<title>Data Sciences|Bigdata</title>
		<link rel="stylesheet" href="./css/style.css">
		<link rel="shortcut icon" type="image/x-icon" href="img/mlp7.jpg" />
		<script src="https://d3js.org/d3.v3.min.js"></script>
		<script type="text/javascript" src="https://latex.codecogs.com/latexit.js"></script>
		
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-116330579-1"></script>
		<script>
 		window.dataLayer = window.dataLayer || [];
  		function gtag(){dataLayer.push(arguments);}
 		gtag('js', new Date());
		gtag('config', 'UA-116330579-1');
		</script>
 </head> 
 <body>
		<header>
			<div class="container">
				<div id="branding">
					<h1><span class="highlight"><a class="branding" href="index.html">DATA SCIENCES</a></span></h1>
				</div>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="python.html">Python</a></li>
						<li><a href="r.html">R</a></li>
						<li><a href="sql.html">SQL</a></li>
						<li><a href="javascript.html">JavaScript</a></li>
						<li class="current"><a href="bigdata.html">Bigdata</a></li>
						<li><a href="about.html">About</a></li>
						<a href="https://twitter.com/ChunlinHe" target="_blank"><img src="img/tweet_icon.png" width="31" height="31" alt="" /></a>
						<a href="https://www.linkedin.com/in/chunlinhe888" target="_blank"><img src="img/in_icon.png" width="31" height="31" alt="" /></a>
					</ul>
				</nav>
			</div>
		</header>

		<section id="main">
			<div class="container">
				<article id="main-col">
					<h1 class="page-title">Big Data</h1>
					<h3>Big data concept</h3>
					<p>
						<em>"Big data is high-volume, high-velocity and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation." -- Gartner (2012)</em>
					</p>
					<p>
						Accordingly, big data have three defining properties/dimensions including <b>(1) volume</b> (quantity), <b>(2) variety</b> (types: structured, semi-structured and unstructured) and <b>(3) velocity</b> (streaming data with speed). The variety of big data implies any of the following types:
						<ul>
							<li><b>Structured data:</b> RDBMS data, easily retrieved through SQL.</li>
							<li><b>Semin-structured data:</b> data in files (xml, json docs, NoSQL database).</li>
							<li><b>Unstructured data:</b> images, videos, text files etc.</li>
						</ul>
					</p>
					<br>

					<h1 class="page-title">Big Data Analytics</h1>
					<h3>Data processing and analytics</h3>
					<p>Data processing and analytics include building and training machine learning models, manipulating data with technology, extracting information from data as well as building data tools, applications, and services. It may consist of the following major steps:
					<ul>
						<li>Framing the problem</li>
						<li>Data acquisition for the problem</li>
						<li>Data wrangling</li>
						<li>Machine learning</li>
						<li>Developing a statistical/mathematical model</li>
						<li>Data visualization</li>
						<li>Communicating the output of the analysis: (1) data report, and (2) data products.</li>
					</ul>
					</p>

					<h3 id="datascience">Python libraries and packages for big data analysis in data sciences</h3>
					<ul>
						<li><b>Data wrangling: pandas</b> (data analysis).</li>
						<li><b>Data exploring: matplotlib</b> (graphics computing).</li>
						<li><b>Developing model: scikit-learn</b> (machine learning: data mining & analysis), <b>NumPy</b> (scientific computing).</li>
						<li><b>Visualization: Bokeh</b> (interactive visualization library for browsers), <b>SciPy</b> (science, math & engineering).</li>
					</ul>


					<h3>The outcomes from big data analysis</h3>
					<ul>
						<li><b>Descriptive:</b> to generate descriptive details from the big data.</li>
						<li><b>Predictive:</b> to create a model from the big data and predict likely outcomes.</li>
						<li><b>Prescriptive:</b> to recommend corresponding actions based on the exiting information.</li>
					</ul>
					<br>

					<h1 class="page-title">Machine Learning</h1>
					<h3>What is machine learning?</h3>
					<p> The term "machine learning" was originally given by Arthur Samuel in 1959, which is "a subset of artificial intelligence (AI) in the field of computer science that often uses statistical techniques to give computers the ability to 'learn' with data, without being explicitly programmed". Arthur Samuel is considered a pioneer of AI and computer gaming from IBM in the United States.
					</p>
	 
					<h3>Machine learning algorithms</h3>
					<p>The machine learning algorithms can be divided into three categories: supervised, unsupervised, and reinforcement algorithms (Fig. on the right). (1) Supervised algorithms: it requires the input and an expected output for the data. The variables from the model are adjusted during the testing process so that the output closes to the expected value/goals. (2) Unsupervised algorithms: it has input but there is no particular outcome expected from the testing phase. The algorithms may cluster the data sets together for different outcomes. (3) Reinforcement algorithms: it trains the algorithms for improvement after each decision, based on its output (success or failure). Listed below are some of the popular machine learning algorithms.
					<ul>
						<li><b>Decision tree:</b> Decision tree algorithm is a type of supervised machine learning where the data continuously get split based on specific parameter, i.e. an output is generated based on the input from the training data. The tree is formed with two entities, namely the decision nodes (where the data is split) and the leaves (the decisions).  In other words, it classifies a set of existing data into different groups according certain attributes, perform a test at each node, through branch judgement, and continuously split the data into additional distinct groups. The tests are made on the existing data, and when new data come in they can be classified to the corresponding groups.<br><br>
						There are two main types of Decision Trees including (1) Classification trees (Yes/No types): The decision variable is categorical; (2) Regression trees (continuous data types): The decision or the outcome variable is continuous such as an actual number. The two common decision tree algorithms include Random Forests (it builds different classifiers using a random subset of attributes and combines them for output) and Boosting Trees (it trains a cascade of trees one on top of others by correcting the mistakes of ones below them).
						</li><br>
						<li><b>Linear regression:</b> Linear regression is used to predict the value of an outcome variable (Y) based on one or more input predictor variables (X). Its objective is to develop a model that the dependent variable (Y) is expressed as a mathematical function of one or more independent variables (X), so that we can use the regression model to predict Y if X is known. The mathematical function can be expressed below (such as simple linear regression with one independent variable):<br>
						<br>
						 
						<img src="https://latex.codecogs.com/gif.latex?Y=\beta&space;_{1}&space;&plus;&space;\beta&space;_{2}X&space;&plus;&space;\varepsilon" title="Y=\beta _{1} + \beta _{2}X + \varepsilon" />
						<br>
						<br>
						where, <img src="https://latex.codecogs.com/gif.latex?\beta&space;_{1}" title="\beta _{1}" /> is the intercept and <img src="https://latex.codecogs.com/gif.latex?\beta&space;_{2}" title="\beta _{2}" /> is the slope, which  are both called <em>regression coefficients</em>. <em>ϵ</em> is the error term for the portion of Y where the regression model cannot explain.<br>
						<br>
						The <em>t-statistic</em> (<img src="https://latex.codecogs.com/gif.latex?\beta&space;-coefficient/Std.Error" title="\beta -coefficient/Std.Error" />) is used to test the significance of the individual regression coefficients while the <em>F-statistic</em> (<img src="https://latex.codecogs.com/gif.latex?MS_{R}/MS_{E}" title="MS_{R}/MS_{E}" />) is for the test of goodness of fit for the regression model, where:<br>
						<img src="https://latex.codecogs.com/gif.latex?MS_{R}=\sum_{i}^{n}(\hat{y}_{i}-\bar{y})/(q-1)=(SS_{T}-SS_{E})/(q-1)" title="MS_{R}=\sum_{i}^{n}(\hat{y}_{i}-\bar{y})/(q-1)=(SS_{T}-SS_{E})/(q-1)" /><br>
						<img src="https://latex.codecogs.com/gif.latex?MS_{E}=SS_{E}/(n-q)" title="MS_{E}=SS_{E}/(n-q)" /> and <img src="https://latex.codecogs.com/gif.latex?MS_{T}=SS_{T}/(n-1)" title="MS_{T}=SS_{T}/(n-1)" />, while <br>
						<img src="https://latex.codecogs.com/gif.latex?SS_{E}=\sum_{i}^{n}(y_{i}-\hat{y}_{i})^{2}," title="SS_{E}=\sum_{i}^{n}(y_{i}-\hat{y}_{i})^{2}," /> 
						<img src="https://latex.codecogs.com/gif.latex?SS_{T}=\sum_{i}^{n}(y_{i}-\bar{y_{i}})^{2}" title="SS_{T}=\sum_{i}^{n}(y_{i}-\bar{y_{i}})^{2}" /><br>
						where, <em>n</em> is the number of observations, <em>q</em> is the number of coefficients, <img src="https://latex.codecogs.com/gif.latex?\hat{y}_{i}" title="\hat{y}_{i}" /> is the fitted value for observation <em>i</em> and <img src="https://latex.codecogs.com/gif.latex?\bar{y}" title="\bar{y}" /> is the mean of Y.<br>
						<br>
						<img src="https://latex.codecogs.com/gif.latex?R^{2}" title="R^{2}" /> and <img src="https://latex.codecogs.com/gif.latex?R^{2}_{adj}" title="R^{2}_{adj}" />:  <img src="https://latex.codecogs.com/gif.latex?R^{2}" title="R^{2}" /> measures the proportion of variation in the dependent (response) variable that is explained by the model. As more independent variables X are included to the new model, they would add to the total variation that was already explained, regardless of their significance levels, which would cause a greater <img src="https://latex.codecogs.com/gif.latex?R^{2}" title="R^{2}" /> value. Thus, the new <img src="https://latex.codecogs.com/gif.latex?R^{2}" title="R^{2}" /> value for the new model needs to be adjusted, namely <img src="https://latex.codecogs.com/gif.latex?R^{2}_{adj}" title="R^{2}_{adj}" />:<br>
						<br>
						<img src="https://latex.codecogs.com/gif.latex?R^{2}=1-SS_{E}/SS_{T}" title="R^{2}=1-SS_{E}/SS_{T}" /> and <img src="https://latex.codecogs.com/gif.latex?R_{adj}^{2}=1-MS_{E}/MS_{T}" title="R_{adj}^{2}=1-MS_{E}/MS_{T}" />, which have the following relationship:<br>
						<img src="https://latex.codecogs.com/gif.latex?R_{adj}^{2}=1-\frac{(1-R^{2})(n-1)}{n-q}" title="R_{adj}^{2}=1-\frac{(1-R^{2})(n-1)}{n-q}" />
						</li><br>
						<li><b>Logistic regression:</b> Logistic regression was developed by the statistician David Cox in 1958 and is one of the popular machine learning algorithms for binary classification. It measures the relationship between the dependent variable and the independent variables by estimating the probabilities using its underlying logistic function which is also called the sigmoid function as shown below:<br>
						<br>
						<img src="https://latex.codecogs.com/gif.latex?y=\frac{1}{1&plus;e^{-x}}=\frac{e^{x}}{e^{x}&plus;1}" title="y=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}" /><br>
						<br>
						The probabilities are then transformed into binary values to make a prediction based on the logistic function. It aims to maximize the likelihood that a random data point is classified correctly into one of the two possible outcomes or dichotomy using Maximum Likelihood Estimation.<br>
						<br>
						</li>
						<li><b>Naive Bayes:</b> It is a simple but powerful classification technique for predictive modeling based on Bayes’ Theorem assuming the independence among predictors or independent variables. The model may include two types of probabilities which may be obtained from the training data: (1) The probability for each class; (2) The conditional probability for each class given a value for each of the independent variables. The probability model is used for making predictions for new data using Bayes Theorem:<br>
						<br>
						<img src="https://latex.codecogs.com/gif.latex?P(A|B)=\frac{P(B|A)P(A)}{P(B)}" title="P(A|B)=\frac{P(B|A)P(A)}{P(B)}" /><br>
						where, A and B are two separate events, and:<br>
						<em>P(A|B)</em> is the conditional probability of the hypothesis that event <em>A</em> occurs , given that the predictor B has occurred (the evidence), which is also called the posterior probability of class (the target <em>A</em>).<br>
						<em>P(A)</em> is the probability of hypothesis <em>A</em> being true, which is also called the prior probability.<br>
						<em>P(B)</em> is the prior probability of the evidence or the predictor <em>B</em> (regardless of the hypothesis).<br>
						<em>P(B|A)</em> is the conditional probability (or likelihood) that event <em>B</em> occurs (the evidence), given that the hypothesis <em>A</em> is true.<br>
						<br>
						Therefore, Bayes’ Theorem (or Bayes’ rule) can also be expressed as:<br>
						<br>
						<img src="https://latex.codecogs.com/gif.latex?posterior=\frac{likelihood&space;*&space;prior}{evidence}" title="posterior=\frac{likelihood * prior}{evidence}" /><br>
						<br>
						</li>
						<li><b>K-nearest neighbors (K-NN):</b> K-NN is one of the simple but robust and widely used classification algorithms for supervised machine learning. It is a non-parametric technique without a need to make any underlying assumptions about the distribution of data. It has many applications including pattern recognition, data mining (including text mining), intrusion detection, finance, medicine and genetics (such as gene assignment in functional genomics according to gene expression profiles), etc. It is also frequently used for other complex classifiers such as artificial neural networks (ANN) and support vector machines (SVM).<br>
						<br>
						This algorithm uses the memorized training data points for the K instances that are most close or similar to the new instance (e.g., distance function) and assigns to it the most common class. In other words, the unknown data point can be predicted for its appropriate classification based on which k data points of the training set are the closest to it using the following Euclidean distance for continuous variables. <br>	
						<br>
						The Euclidean distance between an existing point (<em>a</em>) and a new point (<em>b</em>), i.e. <img src="https://latex.codecogs.com/gif.latex?\overline{ab}" title="\overline{ab}" />, across all input attributes <em>i</em> is given by:
						<br>
						<img src="https://latex.codecogs.com/gif.latex?d(a,b)=d(b,a)=\sqrt{\sum_{i=1}^{k}(a_{i}-b_{i})^{2}}" title="d(a,b)=d(b,a)=\sqrt{\sum_{i=1}^{k}(a_{i}-b_{i})^{2}}" />					
						</li>
						<li><b>K-means:</b> K-means algorithm is a type of unsupervised machine learning for clustering when the data are unlabeled or the data don’t have predefined categories or groups. This algorithm aims to find groups in the data with a variable K being the number of groups. Each of the data points are assigned to one of the K groups based on the similarity of the features provided. Iteration of the K-means clustering algorithm will result in: (a) The centroids of the K clusters that can further be used for labeling new data, (b).Labels of the training data after the data are assigned to different clusters based on the least distance between its centroid and the data point.<br>
						<br>
						The procedure includes: 1 Initializing the k-means: (1) assigning a value to k (for instance, k=3), (2) randomly assign each data point to any of the 3 (or k) clusters and (3) calculate the cluster centroid for each of the clusters. 2: Associating each data point to a cluster by reassigning each point to its closest cluster centroid. 3. Recalculating the centroids for the new clusters. 4. Iterating the procedures 2 and 3 until switching of data points from one cluster to another is no longer needed. By then, the k-means algorithm exits. <br>
						<br>
						For a set of observations, <img src="https://latex.codecogs.com/gif.latex?x_{1},&space;x_{2},...,&space;x_{n}" title="x_{1}, x_{2},..., x_{n}" />, each of them is a dimensional real vector, <em>k</em>-means clustering is to partition the <em>n</em> observations into <em>k</em> (<img src="https://latex.codecogs.com/gif.latex?\leq&space;n" title="\leq n" />) sets of clusters, i.e. <img src="https://latex.codecogs.com/gif.latex?S=\begin{Bmatrix}&space;S_{1},S_{1},...,S_{k}&space;\end{Bmatrix}" title="S=\begin{Bmatrix} S_{1},S_{1},...,S_{k} \end{Bmatrix}" />, in order to minimize the within-cluster sum of squares (WCSS) (thus the variance). Then, for effective clustering, it is essential to find:<br> 
						<br>
						<img src="https://latex.codecogs.com/gif.latex?I=arg\underset{S}{}min{\sum_{i=1}^{k}}\sum_{x\in&space;S_{i}}\left&space;\|x-\mu&space;_{i}&space;\right&space;\|^{2}" title="i=arg\underset{S}{}min{\sum_{i=1}^{k}}\sum_{x\in S_{i}}\left \|x-\mu _{i} \right \|^{2}" /><br>
						where <img src="https://latex.codecogs.com/gif.latex?\mu&space;_{i}" title="\mu _{i}" /> is the mean of all points in <img src="https://latex.codecogs.com/gif.latex?S_{i}" title="S_{i}" />. 
						</li>
					</ul><br>
					</p>	
	 
					<h1 class="page-title">Deep Learning</h1>
					<h3>What is deep learning?</h3>
					<p>Deep learning is a subfield of machine learning using artificial or deep neural networks to train the computer to perform human-like tasks such as image and speech recognition. Deep learning is currently at the cutting edge of machine learning (ML) and artificial intelligence (AI) which has led to the breakthroughs such as audio processing, computer vision as well as self-driving cars, etc.
					</p>
					<p>Most of the deep learning methods use neural network architectures, thus the deep learning models are usually considered as deep neural networks where “deep” relates to the number of hidden layers in the neural network and the Networks may have tens or hundreds of hidden layers.
					</p>		
	 
					<h3>Packages for deep learning</h3>
					<p>Python has a number of deep learning libraries and listed below are the most useful libraries that support different deep learning architectures such as auto-encoders, recurrent neural networks (RNNs), convolutional neural networks (CNNs) and feed-forward networks etc..
					<ul>
						<li><b>Theano:</b> is a low-level and major foundational library with tight integration with NumPy for fast and efficient numerical computation that can be run on the CPU or GPU architectures.</li>
						<li><b>TensorFlow:</b> is a low-level library less mature than Theano for fast numerical computing which is supported by Google for out-of-the-box distributed computing. It is a foundation library for creating Deep Learning models directly or by using wrapper libraries that simplify the process built on top of TensorFlow.</li>
						<li><b>Keras:</b> is a heavyweight wrapper for Theano and Tensorflow. Keras is a high-level neural networks API and is modular for rapid experimentation.</li>
						<li><b>Lasagne:</b> is a lightweight wrapper for Theano for building and training neural networks, which provides the flexibility of Theano and the neural network layers to be used.</li>
						<li><b>MXNet:</b> is a high-level library similar to Keras. The framework provides bindings for multiple languages and support for distributed computing that is designed for efficiency, flexibility and the productivity.</li>
					</ul>
					</p>	
					<hr>
					<br><br>
				</article>
				
				<br><br>
				<aside id="sidebar">
					<div class="navy">
						<img src="bimg/bimg1.jpg" alt="b-icon" height="36%" width="100%" align="right">
						<br><br>
						<h3>What we do</h3>
						<p>This is somewhat an introduction on <em>big data</em> basics and the analytics. The increasing amount of genome sequencing data from various biological species can be a type of big data in terms of computational genomics and data analysis. It is aimed at wet-lab researchers who want to understand big data concept and the analyses and bioinformaticians who are new to <em>big data</em> and want to learn it.</p>
					</div> 
					<br><br><br><br><br><br>
					<div class="purple">
						<a href="bimg/bimg3_ds.jpg" style="text-decoration:none;">
						<img src="bimg/bimg3_ds.jpg" alt="ds" height="300%" width="100%">
						<br>
						<center> <font color="#ffffff")>Data sciences are essential for business<br>(<em>Vincent Granville. 2017. Datasciencecentral.com</em>) </font></center>
						</a>
					</div>
					<br><br><br><br><br><br>
					<div class="purple">
						<a href="bimg/bimg4_ml.jpg" style="text-decoration:none;">
						<img src="bimg/bimg4_ml.jpg" alt="ml" height="300%" width="100%">
						<br>
						<center> <font color="#ffffff")>Machine learning and its applications<br>(Adapted with minor changes from: <em>William Vorhies. 2017. Datasciencecentral.com</em>) </font></center>
						</a>
					</div>
					
				</aside>
			</div>
		</section>
		<br>
		
		<footer>
			<p>Data Sciences, Copyright &copy; 2018 
			<a href="https://twitter.com/ChunlinHe" target="_blank"><img src="img/tweet_icon.png" width="31" height="31" alt="" /></a>
			<a href="https://www.linkedin.com/in/chunlinhe888" target="_blank"><img src="img/in_icon.png" width="31" height="31" alt="" /></a>
			</p>			
			<p id="time"></p>
			<script>
			document.getElementById("time").innerHTML = Date();
			</script>
		</footer>
</body>

</html> 
